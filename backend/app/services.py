import unicodedata
import os
import json
import re
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from pathlib import Path

from dotenv import load_dotenv
from passlib.context import CryptContext
from jose import jwt
from sqlalchemy.orm import Session
from sqlalchemy import inspect
from sqlalchemy.sql import text
from fastapi import HTTPException

import google.generativeai as genai
from google.generativeai.types import GenerationConfig

from app.config import SECRET_KEY, ALGORITHM
from app.models import Empresa

import json
from datetime import date

# === üîê Cargar el archivo .env desde /backend ===
env_path = Path(__file__).resolve().parent.parent / ".env"
load_dotenv(dotenv_path=env_path)

# === üîê Configurar API Key ===
api_key = os.getenv("GOOGLE_API_KEY")
print(f"üîê API Key cargada: {api_key}")
genai.configure(api_key=api_key)




# === ‚öôÔ∏è Configurar el modelo Gemini ===
model = genai.GenerativeModel(
    model_name='gemini-1.5-flash',
    generation_config=GenerationConfig(
        temperature=0.2,
        top_p=1.0,
        max_output_tokens=512,
        stop_sequences=None
    )
)

# === üîí Utilidades de autenticaci√≥n ===
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")
ACCESS_TOKEN_EXPIRE_MINUTES = 30

def hash_password(password: str) -> str:
    return pwd_context.hash(password)

def verify_password(plain_password: str, hashed_password: str) -> bool:
    return pwd_context.verify(plain_password, hashed_password)

def create_access_token(data: dict, expires_delta: timedelta | None = None) -> str:
    to_encode = data.copy()
    expire = datetime.utcnow() + (expires_delta or timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES))
    to_encode.update({"exp": expire})
    token = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
    return token

def normalize_text(text: str) -> str:
    normalized = ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')
    return normalized.lower().strip()

def execute_sql_query(db: Session, query: str) -> List[Dict]:
    try:
        if not query.strip().lower().startswith("select"):
            print(f"Consulta no permitida: {query}")
            return [{"error": "Solo se permiten consultas SELECT."}]
        result = db.execute(text(query))
        columns = result.keys()
        return [dict(zip(columns, row)) for row in result.fetchall()]
    except Exception as e:
        print(f"Error al ejecutar la consulta SQL: {str(e)}")
        return [{"error": f"Error al ejecutar la consulta: {str(e)}"}]

def get_database_schema(db: Session) -> str:
    inspector = inspect(db.bind)
    schema = "La base de datos tiene las siguientes tablas, columnas y relaciones:\n"
    for table_name in inspector.get_table_names():
        if not table_name.startswith(('pg_', 'sql_')):
            schema += f"- Tabla '{table_name}':\n"
            columns = inspector.get_columns(table_name)
            for column in columns:
                schema += f"  - {column['name']} ({column['type'].__class__.__name__}"
                if column.get('primary_key'):
                    schema += ", clave primaria"
                if column.get('nullable'):
                    schema += ", nullable"
                schema += ")\n"
            foreign_keys = inspector.get_foreign_keys(table_name)
            for fk in foreign_keys:
                schema += f"  Relaci√≥n: '{table_name}.{fk['constrained_columns'][0]}' -> '{fk['referred_table']}.{fk['referred_columns'][0]}'\n"
    return schema





# Funci√≥n auxiliar para manejar tipos no serializables
def custom_json_serializer(obj):
    if isinstance(obj, date):
        return obj.isoformat()  # Convierte datetime.date a formato ISO (por ejemplo, "2021-11-05")
    raise TypeError(f"Type {type(obj)} not serializable")


def get_chat_response(db: Session, message: str, history: List[Dict[str, str]] = None):
    try:
        # Configurar el modelo
        model = genai.GenerativeModel('gemini-1.5-flash')

        # Normalizar el mensaje del usuario
        user_input = normalize_text(message)
        print(f"Mensaje recibido: {message}")

        # Construir el historial de la conversaci√≥n
        chat_history = ""
        if history:
            for entry in history:
                if entry.get("user"):
                    chat_history += f"Usuario: {entry['user']}\n"
                if entry.get("assistant"):
                    chat_history += f"Asistente: {entry['assistant']}\n"

        # Obtener la estructura de la base de datos
        db_schema = get_database_schema(db)
        print(f"Esquema de la base de datos: {db_schema}")

        # Paso 1: Gemini analiza la pregunta y genera la consulta SQL
        intent_prompt = (
            "Tu nombre es POLO, un asistente del Parque Industrial Polo 52. Tu objetivo es ayudar a los usuarios respondiendo preguntas sobre las empresas y datos relacionados del parque de manera fluida y natural, tienes que generar una conversacion fluida, como si fueses una persona. "
            "Se te proporcionar√° el esquema de la base de datos, el historial de la conversaci√≥n y la pregunta actual del usuario. "
            "Tu tarea es:\n"
            "1. Analizar la pregunta del usuario y el historial para determinar la intenci√≥n (por ejemplo, ¬øquiere informaci√≥n de una empresa espec√≠fica, de todas las empresas, servicios, contactos, o relaciones entre ellos?). "
            "2. Diferenciar entre 'servicio' y 'servicio polo':\n"
            "   - 'Servicio' se refiere a servicios generales que no son espec√≠ficos del parque, como 'agua', 'residuos', 'electricidad', entre otros. Por defecto, asume que servicios como estos deben buscarse en la tabla 'servicio', vinculados a empresas a trav√©s de 'empresa_servicio' y 'tipo_servicio'. Solo usa esta tabla a menos que el usuario especifique lo contrario.\n"
            "   - 'Servicio polo' se refiere a servicios espec√≠ficos que las empresas contratan dentro del Parque Industrial Polo 52, como 'nave', 'container', 'oficina'. Usa la tabla 'servicio_polo' solo si la pregunta menciona expl√≠citamente 'servicio polo' o ejemplos como 'nave', 'container', 'oficina'.\n"
            "3. Si la pregunta contiene errores de escritura (por ejemplo, 'cuqles' en lugar de 'cu√°les', o 'Jsoefina' en lugar de 'Josefina'), corrige los t√©rminos aproximados usando ILIKE para buscar coincidencias cercanas en las columnas relevantes, pero solo en campos de tipo texto.\n"
            "4. Si la pregunta requiere relacionar varias tablas (por ejemplo, 'qu√© contactos tiene la empresa maxiiiiiii'), usa las relaciones definidas en el esquema (como claves for√°neas) para generar una consulta SQL con JOIN. Identifica las columnas adecuadas para las uniones bas√°ndote en las relaciones del esquema.\n"
            "5. Si falta informaci√≥n clave, responde con un JSON pidiendo aclaraci√≥n: {'needs_more_info': true, 'question': '¬øPuedes especificar de qu√© empresa o servicio quieres informaci√≥n?'}. "
            "6. Si no entiendes la pregunta, responde con un JSON: {'needs_more_info': true, 'question': 'Lo siento, no entiendo tu pregunta. ¬øPuedes reformularla o preguntar algo sobre las empresas del Parque Industrial Polo 52?'}. "
            "7. Si la intenci√≥n es clara, genera una consulta SQL para PostgreSQL basada √∫nicamente en el esquema proporcionado. Usa ILIKE para b√∫squedas de texto insensibles a may√∫sculas/min√∫sculas y solo en campos de tipo texto (como 'nombre'). Para campos num√©ricos (como 'cuil'), usa operadores num√©ricos como '='. Usa LEFT JOIN para incluir datos opcionales de tablas relacionadas.\n"
            "8. Responde con un JSON: {'needs_more_info': false, 'sql_query': '...', 'corrected_entity': '...'}. Usa 'corrected_entity' solo si corriges un nombre (por ejemplo, 'corrected_entity': 'Josefina' o 'corrected_entity': 'nave').\n\n"
            "Esquema de la base de datos:\n"
            f"{db_schema}\n\n"
            "Historial de la conversaci√≥n:\n"
            + chat_history +
            "\n\nPregunta actual del usuario: " + user_input
        )

        print(f"Prompt enviado a Gemini para an√°lisis de intenci√≥n y generaci√≥n de SQL: {intent_prompt}")
        intent_response = model.generate_content(intent_prompt)
        intent_response_text = intent_response.text

        # Parsear la respuesta de Gemini como JSON
        try:
            intent_data = json.loads(intent_response_text.strip("```json\n").strip("\n```"))
        except json.JSONDecodeError:
            print(f"Error al parsear la respuesta de Gemini: {intent_response_text}")
            raise HTTPException(status_code=500, detail="Error al procesar la respuesta del modelo.")

        print(f"Respuesta de Gemini: {intent_data}")

        # Paso 2: Si Gemini necesita m√°s informaci√≥n, devolver la pregunta aclaratoria
        if intent_data.get("needs_more_info", False):
            return intent_data["question"]

        # Si la intenci√≥n es clara, ejecutar la consulta SQL
        sql_query = intent_data.get("sql_query")
        if not sql_query:
            return "Lo siento, no se pudo generar una consulta v√°lida para tu pregunta. ¬øPuedes reformularla?"

        # Ejecutar la consulta SQL generada por Gemini
        print(f"Ejecutando consulta SQL: {sql_query}")
        db_results = execute_sql_query(db, sql_query)
        print(f"Resultados de la consulta SQL (crudos): {db_results}")

        # Normalizar los resultados a una lista de diccionarios
        if db_results and not isinstance(db_results, list):
            db_results = [db_results]
        elif not db_results:
            db_results = []

        print(f"Resultados de la consulta SQL (normalizados): {db_results}")

        # Manejar errores en la consulta
        if db_results and "error" in db_results[0]:
            return db_results[0]["error"]

        # Si no hay resultados, devolver un mensaje gen√©rico
        if not db_results:
            entity = intent_data.get("corrected_entity", "desconocida")
            return f"No se encontraron datos para {entity}. ¬øQuieres informaci√≥n sobre otra cosa? Por ejemplo, otra empresa o aspecto del Parque Industrial Polo 52."

       # Convertir los resultados a un formato JSON-like m√°s claro
        db_info = json.dumps(db_results, ensure_ascii=False, default=custom_json_serializer)
        print(f"Datos enviados a Gemini (db_info): {db_info}")

        # Verificar si db_info est√° vac√≠o
        if db_info == "[]":
            print("Advertencia: db_info est√° vac√≠o, Gemini deber√≠a devolver mensaje de no datos.")
        else:
            print("db_info no est√° vac√≠o, Gemini deber√≠a usar los datos para responder.")

        # Preprocesar los resultados para eliminar duplicados y convertir a texto legible
        processed_results = []
        seen = set()
        for item in db_results:
            item_key = tuple(sorted(item.items()))  # Usa una tupla de los pares clave-valor para identificar duplicados
            if item_key not in seen:
                seen.add(item_key)
                processed_results.append(item)

        # Determinar din√°micamente el campo relevante (tomar el primer campo disponible que no sea un ID)
        if processed_results:
            # Obtener las claves del primer diccionario
            first_item = processed_results[0]
            relevant_field = None
            for key in first_item.keys():
                # Ignorar campos que parezcan IDs
                if not key.startswith('id_') and key != 'id':
                    relevant_field = key
                    break
            if not relevant_field:
                relevant_field = list(first_item.keys())[0]  # Si no se encuentra un campo no-ID, usa el primero

            # Extraer los valores del campo relevante
            unique_values = [str(item.get(relevant_field, 'Desconocido')) for item in processed_results]
            results_text = ", ".join(unique_values) if unique_values else "No se encontraron detalles espec√≠ficos"
        else:
            results_text = "No se encontraron resultados"

        # Combinar la pregunta del usuario con los resultados preprocesados
        input_text = f"Pregunta del usuario: {user_input}\nResultados de la base de datos: {results_text}"

        # Paso 3: Gemini genera la respuesta final con los datos preprocesados
        final_prompt = (
            "Tu nombre es POLO, un asistente conversacional del Parque Industrial Polo 52. "
            "Tu tarea es usar la informaci√≥n proporcionada sobre la pregunta del usuario y los resultados de la base de datos para responder de manera clara, simple y f√°cil de entender, sin mostrar detalles t√©cnicos de la consulta SQL al usuario final. "
            "Instrucciones:\n"
            "- La informaci√≥n incluye la pregunta del usuario y los resultados de la base de datos como texto. Los resultados son la respuesta directa a la pregunta del usuario y ya han sido procesados para ser relevantes.\n"
            "- Si los resultados de la base de datos son distintos de 'No se encontraron resultados', significa que hay datos v√°lidos para responder. Usa esos datos directamente para responder a la pregunta.\n"
            "- Estructura la respuesta de manera amigable y organizada. Comienza con una salutaci√≥n ('Hola, soy POLO, tu asistente del Parque Industrial Polo 52.'), presenta la informaci√≥n basada en los resultados (por ejemplo, 'Los tipos de servicio de polo disponibles son: coworking, oficina.'), y termina ofreciendo m√°s ayuda ('¬øTe gustar√≠a saber m√°s sobre otro aspecto del Parque Industrial Polo 52?').\n"
            "- Si los resultados de la base de datos son exactamente 'No se encontraron resultados', responde with: 'Hola, soy POLO, tu asistente del Parque Industrial Polo 52. No se encontraron resultados para tu solicitud. ¬øTe gustar√≠a saber m√°s?'.\n"
            "- Adapta la respuesta al contexto de la pregunta del usuario (por ejemplo, si menciona 'servicio', enf√≥cate en servicios). Usa el historial de conversaci√≥n para dar contexto adicional si es necesario.\n"
            f"Informaci√≥n proporcionada:\n{input_text}\n"
            f"Historial de la conversaci√≥n:\n{chat_history}"
        )

        print(f"Prompt enviado a Gemini para respuesta final: {final_prompt}")
        final_response = model.generate_content(final_prompt)
        final_response_text = final_response.text
        print(f"Respuesta final de Gemini: {final_response_text}")
        return final_response_text

    except Exception as e:
        print(f"Error al procesar el mensaje: {str(e)}")
        return f"Error al procesar el mensaje: {str(e)}"